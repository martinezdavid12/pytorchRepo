{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 01 - Pytorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Workflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Imports ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn # nn contains all of pytorch's buildign blocks for neural networks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Check PyTorch version\n",
    "torch.__version__\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building first model ###\n",
    "- Using Torch NN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data (preparing and loading) ##\n",
    "Data can be many things!\n",
    "- Excel sheet\n",
    "- Images\n",
    "- Videos\n",
    "- Audio\n",
    "\n",
    "Machine Learning is a game of two parts: \n",
    "1. Get data into some numerical representation\n",
    "2. Build a model to lean patterns in that numerical representation\n",
    "\n",
    "Inputs &rarr; Numerical Encoing &rarr; Learns representation(network) &rarr; Representation Outputs &rarr; Interpret Outputs\n",
    "\n",
    "To show this, start with linear regression."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Linear regression to make a straight line with known paramters ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create *known* parameters :    Y = A + BX\n",
    "# Let weight be B  and bias be A\n",
    "weight = 0.7\n",
    "bias = 0.3\n",
    "\n",
    "#Create\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.02\n",
    "X = torch.arange(start, end, step).unsqueeze(dim=1) #gets rid of extra container-like bracket\n",
    "y = weight * X + bias    #essentially  y = Bx+ A\n",
    "\n",
    "X[:10], y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X), len(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into training and test sets (one of the most important concepts in machine learning in general) ###\n",
    "\n",
    "Typically:\n",
    "* Traning set is about 60-80% (always used in ML)\n",
    "* Validation set is about 10-20%  (not necessary for all forms of ML)\n",
    "* Testing set is about 10-20%  (always used in ML)\n",
    "\n",
    "\n",
    "Create training and test data set with our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train/test split\n",
    "train_split = int(0.8 * len(X))\n",
    "X_train, y_train = X[:train_split], y[:train_split]\n",
    "X_test, y_test = X[train_split:], y[train_split:]\n",
    "#Validate training - to - testing ratio\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Plot Function: ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(train_data=X_train, \n",
    "                     train_labels=y_train,\n",
    "                     test_data=X_test,\n",
    "                     test_labels=y_test,\n",
    "                     predictions=None):\n",
    "                     \n",
    "    \"\"\"\n",
    "    Plots training data, test data and compares predictions.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,7))\n",
    "\n",
    "    #Plot training data in blue\n",
    "    plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training Data\")\n",
    "\n",
    "    #Plot test data in green\n",
    "    plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n",
    "\n",
    "    #Check if the model yields a prediction\n",
    "    if predictions is not None:\n",
    "        #Plot the predictions if they exist\n",
    "        plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n",
    "    #Show the Legend\n",
    "    plt.legend(prop={\"size\":14})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build model\n",
    "\n",
    "First Linear regression PyTorch model!\n",
    "\n",
    "what the model does:\n",
    "* Start with random values (weight & bias)\n",
    "* Look at training data and adjust the random values to better represent(or get closer to) the ideal values (the weight and bias values we used to create training and testing data!)\n",
    "\n",
    "How does it do so?\n",
    "Through two main algorithms:\n",
    "1. Gradient Descent\n",
    "2. Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "# Create linear regression model class\n",
    "class LinearRegressionModel(nn.Module): # <- almost everything in PyTorch inherits nn.Module\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(1, \n",
    "                                                 requires_grad=True,\n",
    "                                                 dtype=torch.float))\n",
    "        self.bias = nn.Parameter(torch.randn(1, \n",
    "                                                 requires_grad=True,\n",
    "                                                 dtype=torch.float))  \n",
    "        #Forward emthod to define the computation in the model\n",
    "        def forward(self, x: torch.Tensor) -> torch.Tensor: # <- \"x\" is the input data\n",
    "            return self.weights * x + self.bias    #this is effectively the Linear regression formula                                                                           \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward method has to be overridden when using nn.Module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch model building essentials ###\n",
    "* `torch.nn` - contains all of the buildings for computational graphs (a neural network can be considered a computational graph)\n",
    "* `torch.nn.parameter` - what parameters should our model try and elarn, often a PyTorch layer from torch.nn will set these for us\n",
    "* `torch.nn.module` - The base class for all neural network modules, if you subclass it, you should overwrite `forward()`\n",
    "* `torch.optim` - this where the optimizers in PyTorch live, they will help with gradient descent\n",
    "* `def forward()` - All nn.Module subclasses require you t overwrite forward(), this method defines what happens in the forward computation\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the contents of our PyTorch model\n",
    "\n",
    "Now we've created a model, let's see what's inside ...\n",
    "So we can check our model paramters or what's inside our model using `.paramters()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1)\n",
    "b = torch.manual_seed(42)\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "#Create an instance of the model (this is  asubclass of nn.Module)\n",
    "model_0 = LinearRegressionModel()\n",
    "#Check out the parameters\n",
    "list(model_0.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List named paramters\n",
    "model_0.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"ideal values\"\n",
    "weight, bias  # we will try to get the values to \"learn\" these values throuh gradient descent and backpropagation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making rpedictions using `torch.inference_mode()`\n",
    "\n",
    "To check our model's predictive power, let's see how well it predicts `y_test` based on `X_test`.\n",
    "\n",
    "When we pass data through our model, it's going to run it through the `forward()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with model\n",
    "with torch.inference_mode():\n",
    "    y_preds = model_0(X_test)\n",
    "y_preds\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAST TIME STAMP: 5:33:39"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07a481119aadb6992ff0da222930d2b0a8a26de193018adba40c1ff699702765"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
